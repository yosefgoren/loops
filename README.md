## Key Notes
### Producing The Dataset
How to run:
* The repository should be cloned, alon with all of it's submodules (`--recurisve`).
* The `make` script behaves like a makefile and serves as the main script for generating the initial dataset.
* All python scripts/code in this project (excluding anything in a submodule) expects to be run as a part of a 'python module' with the root directory being the root directory of this repository.
    For example, if you want to run the `validation/analyze_valid.py` script, you should run it like `python -m validation.analyze_valid` from the repo's root directory.

### OpenAI API Usage
One method used within this project is to use openAI's API to programatically use the output of an LLM.
This is used both as a tool when generating datasets, and in the validation process.
The parts using the API assume there is a file named `api_key.txt` in the `utils` directory containing an API key which will be used by the project's scripts to access the API.


## Project Architecture
Currently, the main parts of the project are:
### Base Dataset Generation
Generating the base dataset can be done by using the `make` script.

The process of generating the dataset is as follows:
The following steps are done for each target executable:
- parsing the source files for finding scope locations
- looking through the scopes to find ones eligable to be timing measurment targets (most for loops will be).
- modifying the source code to enable measuring the loop's duration at runtime.
- building the modified code
- running the modified code: due to code modification - will executable generates a file logging timestamps from the code
- collecting the output trace of the code by parsing the log file generated by the executable

The next steps are done on the aggregation of results yielded on all of the target executables:
- finalizing the dataset by normalizing the runtimes with respect to each-other and generating aggregate files in the required format
- packing the results by zipping the generated files

### Dataset Augmentation
The base dataset may be expanded by augmenting samples of it.
This is essentially done using openai's API and some prompt engineering.
The scripts for achiving this 